{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "temporal_data_path = os.path.join(data_dir, 'train.csv')\n",
    "mobility_data_path = os.path.join(data_dir, 'graph_round2.csv')\n",
    "\n",
    "temporal_data = pd.read_csv(temporal_data_path)\n",
    "temporal_data.describe()\n",
    "# train_round2.csv 04/12/2020 to 11/22/2020.\n",
    "# train.csv 04/12/2020 to 08/31/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(np.unique(temporal_data['Province_State']))\n",
    "state_cum_temporal_data = dict.fromkeys(states, None)\n",
    "state_temporal_data = dict.fromkeys(states, None)\n",
    "# state_cum_temporal_scaler = defaultdict(StandardScaler)\n",
    "# state_temporal_scaler = defaultdict(StandardScaler)\n",
    "dropped_attr = ['Date',\n",
    "                'Active',\n",
    "                'ID', \n",
    "                'Province_State', \n",
    "                'Incident_Rate', \n",
    "                'Recovered', \n",
    "                'People_Tested', \n",
    "                'People_Hospitalized', \n",
    "                'Mortality_Rate', \n",
    "                'Testing_Rate', \n",
    "                'Hospitalization_Rate']\n",
    "\n",
    "for s in states:\n",
    "    df_filter = temporal_data['Province_State'] == s\n",
    "    state_df = temporal_data[df_filter]\n",
    "    # Daily difference data\n",
    "    state_temporal_data[s] = state_df.drop(dropped_attr, 1)\n",
    "    for col in state_temporal_data[s]:\n",
    "        data = state_temporal_data[s][col].tolist()\n",
    "        diff = [i - j for i, j in zip(data, [*[data[0]], *data[:len(data) - 1]])]\n",
    "        state_temporal_data[s][col] = diff\n",
    "    save_columns = state_temporal_data[s].columns\n",
    "#     state_temporal_data[s] = state_temporal_scaler[s].fit_transform(state_temporal_data[s])\n",
    "    state_temporal_data[s] = pd.DataFrame(state_temporal_data[s], columns=save_columns)\n",
    "    # Daily cumulative data\n",
    "    state_cum_temporal_data[s] = state_df.drop(dropped_attr, 1)\n",
    "    save_columns = state_cum_temporal_data[s].columns\n",
    "#     state_cum_temporal_data[s] = state_cum_temporal_scaler[s].fit_transform(state_cum_temporal_data[s])\n",
    "    state_cum_temporal_data[s] = pd.DataFrame(state_cum_temporal_data[s], columns=save_columns)\n",
    "    \n",
    "print(state_temporal_data['California'])\n",
    "print(state_cum_temporal_data['California'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# window_size is use n-1 days to predict nth day\n",
    "window_size = 22\n",
    "forecast_days = 26\n",
    "\n",
    "# test model for current_state\n",
    "current_state = 'Georgia'\n",
    "data_training = state_temporal_data[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_training = scaler.fit_transform(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "data_training_np = np.array(data_training)\n",
    "\n",
    "for i in range(data_training.shape[0] - window_size):\n",
    "    X_train.append(data_training[i : i + window_size])\n",
    "    y_train.append(data_training_np[i + window_size])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(LSTM(units=10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(units=20, return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(units = 2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'mean_squared_logarithmic_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_queue = X_train[-1:]\n",
    "prediction_queue = np.array(prediction_queue)\n",
    "# prediction_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction_queue should take in this y_pred as the last day and pop the first day in the queue\n",
    "# use model.predict(prediction_queue) with the new prediction_queue to get second day.\n",
    "# need to predict for next forecast_days days.\n",
    "for i in range(forecast_days):  \n",
    "    y_pred = model.predict(prediction_queue)\n",
    "    prediction_queue = np.append(prediction_queue, y_pred)\n",
    "    prediction_queue = np.delete(prediction_queue, 0)\n",
    "    prediction_queue = np.delete(prediction_queue, 0)\n",
    "    prediction_queue = prediction_queue.reshape(1,int(prediction_queue.shape[0]/2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case_forecast = []\n",
    "y_death_forecast = []\n",
    "\n",
    "for i in range(forecast_days):\n",
    "    y_case_forecast.append(prediction_queue[0][i + window_size - forecast_days][0])\n",
    "    y_death_forecast.append(prediction_queue[0][i + window_size - forecast_days][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_scale = 1/scaler.scale_[0]\n",
    "death_scale = 1/scaler.scale_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case_forecast = [case_scale * i for i in y_case_forecast]\n",
    "y_death_forecast = [death_scale * i for i in y_death_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day_case = state_cum_temporal_data[current_state]['Confirmed'].iloc[-1]\n",
    "last_day_death = state_cum_temporal_data[current_state]['Deaths'].iloc[-1]\n",
    "\n",
    "y_cum_case_forecast = []\n",
    "y_cum_death_forecast = []\n",
    "\n",
    "cum_case = last_day_case\n",
    "cum_death = last_day_death\n",
    "for case in y_case_forecast:\n",
    "    cum_case += case\n",
    "    y_cum_case_forecast.append(cum_case)\n",
    "    \n",
    "for death in y_death_forecast:\n",
    "    cum_death += death\n",
    "    y_cum_death_forecast.append(cum_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cum_case_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cum_death_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a RNN with LSTM for Every State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# window_size is use n-1 days to predict nth day\n",
    "window_size = 22\n",
    "forecast_days = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = state_temporal_data.keys()\n",
    "\n",
    "def get_training_data(state):\n",
    "    state_temporal_train = state_temporal_data[state]\n",
    "    scaler = MinMaxScaler()\n",
    "    state_temporal_train = scaler.fit_transform(state_temporal_train)\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    state_temporal_train_np = np.array(state_temporal_train)\n",
    "\n",
    "    for i in range(state_temporal_train.shape[0] - window_size):\n",
    "        X_train.append(state_temporal_train[i : i + window_size])\n",
    "        y_train.append(state_temporal_train_np[i + window_size])\n",
    "\n",
    "    return np.array(X_train), np.array(y_train), scaler\n",
    "    \n",
    "\n",
    "def get_model(X_train):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(LSTM(units=40, activation = 'relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(units=80, activation = 'relu', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=100, activation = 'relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(units = 2))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_prediction_queue(X_train):\n",
    "    prediction_queue = X_train[-1:]\n",
    "    return np.array(prediction_queue)\n",
    "\n",
    "\n",
    "def predict(model, queue, scaler):\n",
    "    prediction_queue = queue\n",
    "    \n",
    "    for i in range(forecast_days):  \n",
    "        y_pred = model.predict(prediction_queue)\n",
    "        prediction_queue = np.append(prediction_queue, y_pred)\n",
    "        prediction_queue = np.delete(prediction_queue, 0)\n",
    "        prediction_queue = np.delete(prediction_queue, 0)\n",
    "        prediction_queue = prediction_queue.reshape(1, prediction_queue.shape[0] // 2, 2)\n",
    "        \n",
    "    y_case_forecast = []\n",
    "    y_death_forecast = []\n",
    "\n",
    "    for i in range(forecast_days):\n",
    "        y_case_forecast.append(prediction_queue[0][i + window_size - forecast_days][0])\n",
    "        y_death_forecast.append(prediction_queue[0][i + window_size - forecast_days][1])\n",
    "        \n",
    "    case_scale = 1 / scaler.scale_[0]\n",
    "    death_scale = 1 / scaler.scale_[1]\n",
    "    \n",
    "    return [case_scale * i for i in y_case_forecast], [death_scale * i for i in y_death_forecast]\n",
    "\n",
    "\n",
    "def get_cum_forecast(state, y_case_forecast, y_death_forecast):\n",
    "    last_day_case = state_cum_temporal_data[state]['Confirmed'].iloc[-1]\n",
    "    last_day_death = state_cum_temporal_data[state]['Deaths'].iloc[-1]\n",
    "\n",
    "    y_cum_case_forecast = []\n",
    "    y_cum_death_forecast = []\n",
    "\n",
    "    cum_case = last_day_case\n",
    "    cum_death = last_day_death\n",
    "    for case in y_case_forecast:\n",
    "        cum_case += case\n",
    "        y_cum_case_forecast.append(cum_case)\n",
    "\n",
    "    for death in y_death_forecast:\n",
    "        cum_death += death\n",
    "        y_cum_death_forecast.append(cum_death)\n",
    "        \n",
    "    return y_cum_case_forecast, y_cum_death_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted = {}\n",
    "\n",
    "for state in states:\n",
    "    X_train, y_train, scaler = get_training_data(state)\n",
    "\n",
    "    model = get_model(X_train)\n",
    "    model.compile(optimizer='adam', loss = 'mean_squared_logarithmic_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "    \n",
    "    y_case_forecast, y_death_forecast = predict(model, get_prediction_queue(X_train), scaler)\n",
    "    y_cum_case_forecast, y_cum_death_forecast = get_cum_forecast(state, y_case_forecast, y_death_forecast)\n",
    "    \n",
    "    forecasted[state] = (y_cum_case_forecast, y_cum_death_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastID = [x for x in range(forecast_days * 50)]\n",
    "deaths = []\n",
    "confirmed = []\n",
    "\n",
    "for i in range(forecast_days):\n",
    "    for s in states:\n",
    "        print(f\"Day {i+1}/26, {s}\")\n",
    "        confirmed.append(forecasted[s][0][i])\n",
    "        deaths.append(forecasted[s][1][i])\n",
    "        \n",
    "final = pd.DataFrame(list(zip(forecastID, confirmed, deaths)), \n",
    "               columns =['ForecastID', 'Confirmed', 'Deaths']) \n",
    "\n",
    "final.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
