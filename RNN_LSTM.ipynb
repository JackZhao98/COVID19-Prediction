{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/test_round2.csv\n",
      "./data/train_round2.csv\n",
      "./data/graph_round2.csv\n",
      "./data/test.csv\n",
      "./data/graph.csv\n",
      "./data/submission_round2.csv\n",
      "./data/submission.csv\n",
      "./data/train.csv\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>People_Tested</th>\n",
       "      <th>People_Hospitalized</th>\n",
       "      <th>Mortality_Rate</th>\n",
       "      <th>Testing_Rate</th>\n",
       "      <th>Hospitalization_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11250.000000</td>\n",
       "      <td>1.125000e+04</td>\n",
       "      <td>11250.000000</td>\n",
       "      <td>9419.000000</td>\n",
       "      <td>1.125000e+04</td>\n",
       "      <td>11250.000000</td>\n",
       "      <td>1.055000e+04</td>\n",
       "      <td>5047.000000</td>\n",
       "      <td>10550.000000</td>\n",
       "      <td>11250.000000</td>\n",
       "      <td>5047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5624.500000</td>\n",
       "      <td>9.583625e+04</td>\n",
       "      <td>3057.178311</td>\n",
       "      <td>40276.189086</td>\n",
       "      <td>5.907248e+04</td>\n",
       "      <td>1357.938709</td>\n",
       "      <td>1.161268e+06</td>\n",
       "      <td>6250.044185</td>\n",
       "      <td>3.300561</td>\n",
       "      <td>20085.845676</td>\n",
       "      <td>12.197144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3247.739599</td>\n",
       "      <td>1.516621e+05</td>\n",
       "      <td>5377.615328</td>\n",
       "      <td>81746.843553</td>\n",
       "      <td>1.243728e+05</td>\n",
       "      <td>1199.193935</td>\n",
       "      <td>2.031813e+06</td>\n",
       "      <td>13880.694435</td>\n",
       "      <td>2.039538</td>\n",
       "      <td>19319.956138</td>\n",
       "      <td>5.255183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.700000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.890000e+02</td>\n",
       "      <td>32.771004</td>\n",
       "      <td>5.459000e+03</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>460.300152</td>\n",
       "      <td>2.302896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2812.250000</td>\n",
       "      <td>1.054600e+04</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>3030.500000</td>\n",
       "      <td>4.599500e+03</td>\n",
       "      <td>374.587118</td>\n",
       "      <td>1.579640e+05</td>\n",
       "      <td>595.000000</td>\n",
       "      <td>1.708176</td>\n",
       "      <td>5761.788808</td>\n",
       "      <td>8.387037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5624.500000</td>\n",
       "      <td>4.175800e+04</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>10376.000000</td>\n",
       "      <td>1.579100e+04</td>\n",
       "      <td>1045.850447</td>\n",
       "      <td>4.901655e+05</td>\n",
       "      <td>2068.000000</td>\n",
       "      <td>2.802207</td>\n",
       "      <td>15172.226199</td>\n",
       "      <td>11.351161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8436.750000</td>\n",
       "      <td>1.174958e+05</td>\n",
       "      <td>3387.000000</td>\n",
       "      <td>48028.000000</td>\n",
       "      <td>5.289675e+04</td>\n",
       "      <td>2046.182244</td>\n",
       "      <td>1.247765e+06</td>\n",
       "      <td>6134.500000</td>\n",
       "      <td>4.425834</td>\n",
       "      <td>27209.961598</td>\n",
       "      <td>15.396088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11249.000000</td>\n",
       "      <td>1.153529e+06</td>\n",
       "      <td>34319.000000</td>\n",
       "      <td>913796.000000</td>\n",
       "      <td>1.095798e+06</td>\n",
       "      <td>9537.675412</td>\n",
       "      <td>1.956515e+07</td>\n",
       "      <td>89995.000000</td>\n",
       "      <td>9.741481</td>\n",
       "      <td>134755.702292</td>\n",
       "      <td>38.501190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID     Confirmed        Deaths      Recovered        Active  \\\n",
       "count  11250.000000  1.125000e+04  11250.000000    9419.000000  1.125000e+04   \n",
       "mean    5624.500000  9.583625e+04   3057.178311   40276.189086  5.907248e+04   \n",
       "std     3247.739599  1.516621e+05   5377.615328   81746.843553  1.243728e+05   \n",
       "min        0.000000  2.700000e+02      0.000000       0.000000 -9.890000e+02   \n",
       "25%     2812.250000  1.054600e+04    273.000000    3030.500000  4.599500e+03   \n",
       "50%     5624.500000  4.175800e+04   1031.000000   10376.000000  1.579100e+04   \n",
       "75%     8436.750000  1.174958e+05   3387.000000   48028.000000  5.289675e+04   \n",
       "max    11249.000000  1.153529e+06  34319.000000  913796.000000  1.095798e+06   \n",
       "\n",
       "       Incident_Rate  People_Tested  People_Hospitalized  Mortality_Rate  \\\n",
       "count   11250.000000   1.055000e+04          5047.000000    10550.000000   \n",
       "mean     1357.938709   1.161268e+06          6250.044185        3.300561   \n",
       "std      1199.193935   2.031813e+06         13880.694435        2.039538   \n",
       "min        32.771004   5.459000e+03            10.000000        0.000000   \n",
       "25%       374.587118   1.579640e+05           595.000000        1.708176   \n",
       "50%      1045.850447   4.901655e+05          2068.000000        2.802207   \n",
       "75%      2046.182244   1.247765e+06          6134.500000        4.425834   \n",
       "max      9537.675412   1.956515e+07         89995.000000        9.741481   \n",
       "\n",
       "        Testing_Rate  Hospitalization_Rate  \n",
       "count   11250.000000           5047.000000  \n",
       "mean    20085.845676             12.197144  \n",
       "std     19319.956138              5.255183  \n",
       "min       460.300152              2.302896  \n",
       "25%      5761.788808              8.387037  \n",
       "50%     15172.226199             11.351161  \n",
       "75%     27209.961598             15.396088  \n",
       "max    134755.702292             38.501190  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "temporal_data_path = os.path.join(data_dir, 'train_round2.csv')\n",
    "mobility_data_path = os.path.join(data_dir, 'graph_round2.csv')\n",
    "\n",
    "temporal_data = pd.read_csv(temporal_data_path)\n",
    "temporal_data.describe()\n",
    "#  training data from 04/12/2020 to 11/22/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Confirmed  Deaths\n",
      "4              0       0\n",
      "54          1136      74\n",
      "104         1425      53\n",
      "154         1330      93\n",
      "204          991      96\n",
      "...          ...     ...\n",
      "11004      11755     106\n",
      "11054      13134      89\n",
      "11104      12576      98\n",
      "11154      15685      32\n",
      "11204       9089      54\n",
      "\n",
      "[225 rows x 2 columns]\n",
      "       Confirmed  Deaths\n",
      "4          22795     640\n",
      "54         23931     714\n",
      "104        25356     767\n",
      "154        26686     860\n",
      "204        27677     956\n",
      "...          ...     ...\n",
      "11004    1064040   18453\n",
      "11054    1077174   18542\n",
      "11104    1089750   18640\n",
      "11154    1105435   18672\n",
      "11204    1114524   18726\n",
      "\n",
      "[225 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "states = list(np.unique(temporal_data['Province_State']))\n",
    "state_cum_temporal_data = dict.fromkeys(states, None)\n",
    "state_temporal_data = dict.fromkeys(states, None)\n",
    "# state_cum_temporal_scaler = defaultdict(StandardScaler)\n",
    "# state_temporal_scaler = defaultdict(StandardScaler)\n",
    "dropped_attr = ['Date',\n",
    "                'Active',\n",
    "                'ID', \n",
    "                'Province_State', \n",
    "                'Incident_Rate', \n",
    "                'Recovered', \n",
    "                'People_Tested', \n",
    "                'People_Hospitalized', \n",
    "                'Mortality_Rate', \n",
    "                'Testing_Rate', \n",
    "                'Hospitalization_Rate']\n",
    "\n",
    "for s in states:\n",
    "    df_filter = temporal_data['Province_State'] == s\n",
    "    state_df = temporal_data[df_filter]\n",
    "    # Daily difference data\n",
    "    state_temporal_data[s] = state_df.drop(dropped_attr, 1)\n",
    "    for col in state_temporal_data[s]:\n",
    "        data = state_temporal_data[s][col].tolist()\n",
    "        diff = [i - j for i, j in zip(data, [*[data[0]], *data[:len(data) - 1]])]\n",
    "        state_temporal_data[s][col] = diff\n",
    "    save_columns = state_temporal_data[s].columns\n",
    "#     state_temporal_data[s] = state_temporal_scaler[s].fit_transform(state_temporal_data[s])\n",
    "    state_temporal_data[s] = pd.DataFrame(state_temporal_data[s], columns=save_columns)\n",
    "    # Daily cumulative data\n",
    "    state_cum_temporal_data[s] = state_df.drop(dropped_attr, 1)\n",
    "    save_columns = state_cum_temporal_data[s].columns\n",
    "#     state_cum_temporal_data[s] = state_cum_temporal_scaler[s].fit_transform(state_cum_temporal_data[s])\n",
    "    state_cum_temporal_data[s] = pd.DataFrame(state_cum_temporal_data[s], columns=save_columns)\n",
    "    \n",
    "print(state_temporal_data['California'])\n",
    "print(state_cum_temporal_data['California'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# window_size is use n-1 days to predict nth day\n",
    "window_size = 22\n",
    "forecast_days = 7\n",
    "\n",
    "# test model for current_state\n",
    "current_state = 'Georgia'\n",
    "data_training = state_temporal_data[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_training = scaler.fit_transform(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "data_training_np = np.array(data_training)\n",
    "\n",
    "for i in range(data_training.shape[0] - window_size):\n",
    "    X_train.append(data_training[i : i + window_size])\n",
    "    y_train.append(data_training_np[i + window_size])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 22, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 22, 10)            520       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 10)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 3,042\n",
      "Trainable params: 3,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(LSTM(units=10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(units=20, return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(units = 2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203 samples\n",
      "Epoch 1/50\n",
      "203/203 [==============================] - 12s 61ms/sample - loss: 0.0084\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0064\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0058\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0056\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0055\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0045\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0039\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0037\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0036\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0036\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0036\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0035\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0036\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0034\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0034\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0034\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0033\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0034\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0033\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0032\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0033\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0033\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0033\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0032\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0032\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0031\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0031\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0031\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0031\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0031\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0031\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0031\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 1s 4ms/sample - loss: 0.0031\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0031\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 1s 3ms/sample - loss: 0.0032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84df4a2d68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss = 'mean_squared_logarithmic_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_queue = X_train[-1:]\n",
    "prediction_queue = np.array(prediction_queue)\n",
    "# prediction_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction_queue should take in this y_pred as the last day and pop the first day in the queue\n",
    "# use model.predict(prediction_queue) with the new prediction_queue to get second day.\n",
    "# need to predict for next forecast_days days.\n",
    "for i in range(forecast_days):  \n",
    "    y_pred = model.predict(prediction_queue)\n",
    "    prediction_queue = np.append(prediction_queue, y_pred)\n",
    "    prediction_queue = np.delete(prediction_queue, 0)\n",
    "    prediction_queue = np.delete(prediction_queue, 0)\n",
    "    prediction_queue = prediction_queue.reshape(1,int(prediction_queue.shape[0]/2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case_forecast = []\n",
    "y_death_forecast = []\n",
    "\n",
    "for i in range(forecast_days):\n",
    "    y_case_forecast.append(prediction_queue[0][i + window_size - forecast_days][0])\n",
    "    y_death_forecast.append(prediction_queue[0][i + window_size - forecast_days][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_scale = 1/scaler.scale_[0]\n",
    "death_scale = 1/scaler.scale_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case_forecast = [case_scale * i for i in y_case_forecast]\n",
    "y_death_forecast = [death_scale * i for i in y_death_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day_case = state_cum_temporal_data[current_state]['Confirmed'].iloc[-1]\n",
    "last_day_death = state_cum_temporal_data[current_state]['Deaths'].iloc[-1]\n",
    "\n",
    "y_cum_case_forecast = []\n",
    "y_cum_death_forecast = []\n",
    "\n",
    "cum_case = last_day_case\n",
    "cum_death = last_day_death\n",
    "for case in y_case_forecast:\n",
    "    cum_case += case\n",
    "    y_cum_case_forecast.append(cum_case)\n",
    "    \n",
    "for death in y_death_forecast:\n",
    "    cum_death += death\n",
    "    y_cum_death_forecast.append(cum_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[452587.0703679025,\n",
       " 456063.4661125466,\n",
       " 459573.43573272973,\n",
       " 463116.4421086088,\n",
       " 466302.81951126456,\n",
       " 469534.4814619869,\n",
       " 472808.1929969862]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cum_case_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9246.42029929161,\n",
       " 9294.37745976448,\n",
       " 9342.326924085617,\n",
       " 9390.386852502823,\n",
       " 9435.158864736557,\n",
       " 9480.6154088974,\n",
       " 9526.674355745316]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cum_death_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a RNN with LSTM for Every State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# window_size is use n-1 days to predict nth day\n",
    "window_size = 22\n",
    "forecast_days = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = state_temporal_data.keys()\n",
    "\n",
    "def get_training_data(state):\n",
    "    state_temporal_train = state_temporal_data[state]\n",
    "    scaler = MinMaxScaler()\n",
    "    state_temporal_train = scaler.fit_transform(state_temporal_train)\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    state_temporal_train_np = np.array(state_temporal_train)\n",
    "\n",
    "    for i in range(state_temporal_train.shape[0] - window_size):\n",
    "        X_train.append(state_temporal_train[i : i + window_size])\n",
    "        y_train.append(state_temporal_train_np[i + window_size])\n",
    "\n",
    "    return np.array(X_train), np.array(y_train), scaler\n",
    "    \n",
    "\n",
    "def get_model(X_train):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(LSTM(units = 60, activation = 'relu', return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units = 80, activation = 'relu', return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units = 120, activation = 'relu', return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units = 200, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(units = 1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_prediction_queue(X_train):\n",
    "    prediction_queue = X_train[-1:]\n",
    "    return np.array(prediction_queue)\n",
    "\n",
    "\n",
    "def predict(model, queue, scaler):\n",
    "    prediction_queue = queue\n",
    "    \n",
    "    for i in range(forecast_days):  \n",
    "        y_pred = model.predict(prediction_queue)\n",
    "        prediction_queue = np.append(prediction_queue, y_pred)\n",
    "        prediction_queue = np.delete(prediction_queue, 0)\n",
    "        prediction_queue = np.delete(prediction_queue, 0)\n",
    "        prediction_queue = prediction_queue.reshape(1, prediction_queue.shape[0] // 2, 2)\n",
    "        \n",
    "    y_case_forecast = []\n",
    "    y_death_forecast = []\n",
    "\n",
    "    for i in range(forecast_days):\n",
    "        y_case_forecast.append(prediction_queue[0][i + window_size - forecast_days][0])\n",
    "        y_death_forecast.append(prediction_queue[0][i + window_size - forecast_days][1])\n",
    "        \n",
    "    case_scale = 1 / scaler.scale_[0]\n",
    "    death_scale = 1 / scaler.scale_[1]\n",
    "    \n",
    "    return [case_scale * i for i in y_case_forecast], [death_scale * i for i in y_death_forecast]\n",
    "\n",
    "\n",
    "def get_cum_forecast(y_case_forecast, y_death_forecast):\n",
    "    last_day_case = state_cum_temporal_data[current_state]['Confirmed'].iloc[-1]\n",
    "    last_day_death = state_cum_temporal_data[current_state]['Deaths'].iloc[-1]\n",
    "\n",
    "    y_cum_case_forecast = []\n",
    "    y_cum_death_forecast = []\n",
    "\n",
    "    cum_case = last_day_case\n",
    "    cum_death = last_day_death\n",
    "    for case in y_case_forecast:\n",
    "        cum_case += case\n",
    "        y_cum_case_forecast.append(cum_case)\n",
    "\n",
    "    for death in y_death_forecast:\n",
    "        cum_death += death\n",
    "        y_cum_death_forecast.append(cum_death)\n",
    "        \n",
    "    return y_cum_case_forecast, y_cum_death_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0556\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0182\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0163\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0134\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0128\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0126\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0119\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0116\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0113\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0110\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0110\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0108\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0111\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0108\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0106\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0109\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0111\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0123\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0111\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0111\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0112\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0109\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0109\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0109\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0104\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0108\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0111\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.0105\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0113\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0109\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0108\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0106\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0111\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0111\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0113\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0111\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0115\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0107\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0107\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0105\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0108\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0108\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0112\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0118\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0115\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0105\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0108\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0104\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0105\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 43 into shape (1,21,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e685c31cfdab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my_case_forecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_death_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_prediction_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_cum_case_forecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cum_death_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cum_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_case_forecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_death_forecast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d0356bc325f7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, queue)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprediction_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprediction_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mprediction_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0my_case_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 43 into shape (1,21,2)"
     ]
    }
   ],
   "source": [
    "forecasted = {}\n",
    "\n",
    "for state in states:\n",
    "    X_train, y_train, scaler = get_training_data(state)\n",
    "\n",
    "    model = get_model(X_train)\n",
    "    model.compile(optimizer='adam', loss = 'mean_squared_logarithmic_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "    \n",
    "    y_case_forecast, y_death_forecast = predict(model, get_prediction_queue(X_train), scaler)\n",
    "    y_cum_case_forecast, y_cum_death_forecast = get_cum_forecast(y_case_forecast, y_death_forecast)\n",
    "    \n",
    "    forecasted[state] = (y_cum_case_forecast, y_cum_death_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
